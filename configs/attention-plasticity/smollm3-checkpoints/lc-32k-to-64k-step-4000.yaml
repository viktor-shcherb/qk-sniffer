dataset:
  path: viktoroo/longbench-pro-128k-plus
  name: null
  split: test
  text_column: text
  id_column: id
  max_samples: null

model:
  name: HuggingFaceTB/SmolLM3-3B-checkpoints
  revision: lc-32k-to-64k-step-4000
  dtype: bfloat16
  device_map: auto
  trust_remote_code: false

tokenizer:
  name: HuggingFaceTB/SmolLM3-3B-checkpoints
  max_length: 65536
  padding: longest

inference:
  batch_size: 1
  autocast_dtype: bfloat16

capture:
  capture_queries: true
  capture_keys: true
  layers: null
  heads: null
  head_sampling:
    count: 300
    seed: 0
  full_attention_only: true
  capture_pre_rope: false
  min_bucket_size: 8192
  sampler:
    type: uniform
    base_rate: 1.0

output:
  data_root: data/attention-plasticity
  readme_path: README.md
  hf_repo_id: viktoroo/sniffed-qk
  hf_branch: smollm3-3b-lc-32k-to-64k-step-4000-longbench-pro-128k-plus
