---
configs:
- config_name: all
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/*/*.parquet
  default: true
- config_name: layer00
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h*/*.parquet
- config_name: l00h00
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h00*/*.parquet
- config_name: l00h01
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h01*/*.parquet
- config_name: l00h02
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h02*/*.parquet
- config_name: l00h03
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h03*/*.parquet
- config_name: l00h04
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h04*/*.parquet
- config_name: l00h05
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h05*/*.parquet
- config_name: l00h06
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h06*/*.parquet
- config_name: l00h07
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h07*/*.parquet
- config_name: l00h08
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h08*/*.parquet
- config_name: all_q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/*q/*.parquet
- config_name: layer00_q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h*q/*.parquet
- config_name: l00h00q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h00q/*.parquet
- config_name: l00h01q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h01q/*.parquet
- config_name: l00h02q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h02q/*.parquet
- config_name: l00h03q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h03q/*.parquet
- config_name: l00h04q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h04q/*.parquet
- config_name: l00h05q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h05q/*.parquet
- config_name: l00h06q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h06q/*.parquet
- config_name: l00h07q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h07q/*.parquet
- config_name: l00h08q
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h08q/*.parquet
- config_name: all_k
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/*k/*.parquet
- config_name: layer00_k
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h*k/*.parquet
- config_name: l00h00k
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h00k/*.parquet
- config_name: l00h01k
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h01k/*.parquet
- config_name: l00h02k
  data_files:
  - split: HuggingFaceTB_SmolLM2_135M
    path: data/sniffed-qk/HuggingFaceTB_SmolLM2_135M/l00h02k/*.parquet
models:
- name: HuggingFaceTB/SmolLM2-135M
  url: https://huggingface.co/HuggingFaceTB/SmolLM2-135M
- name: none
  url: https://huggingface.co/none
---

# qk-sniffer

`qk-sniffer` instruments Hugging Face transformer models so every attention layer can stream sampled key/query vectors into a structured dataset. It ships with ready-made patches for Gemma 3 and Llama families, a deterministic sampler, and a CLI that syncs results to the Hugging Face Hub.

## Highlights
- **Zero-touch model overrides** – drop a mirrored module under `models/<name>/modeling_<name>.py` and `patch_modeling_modules()` automatically registers it under `transformers.models.*` before any model loads.
- **Deterministic sampling & metadata** – `LogUniformSampler` keeps attention load manageable by sampling positions per bucket with a reproducible RNG seeded by `(example, layer, head, kind)`. `set_active_example_ids()` lets you persist real document IDs instead of batch indexes.
- **Structured storage with README automation** – `DatasetSaver` writes Parquet shards under sanitized split/config folders, deduplicates `(example_id, position)` pairs, and rewrites the dataset README so `datasets.load_dataset` immediately works against the folder or Hub repo.
- **Turn-key CLI** – `sniff.py` (exported as `sniff-qk`) loads configs from YAML, pulls a Hub dataset snapshot, runs inference with your model/tokenizer choices, captures vectors, and optionally pushes the updated dataset back to the Hub.

## Repository Layout
- `sniff.py` – CLI entrypoint plus helper utilities (`run_inference`, `patch_modeling_modules`, etc.).
- `sniffer/` – reusable runtime (samplers, context managers, dataset capture logic).
- `models/` – mirrored transformer modules with sniffer hooks (Gemma 3, Llama).
- `saver/` – dataset writer + README generator used by the runtime.
- `configs/` – sample YAML configuration (`configs/sample_sniff.yaml`).
- `tests/` – pytest suite covering model patching, sampler behavior, saver layout, and CLI helpers.

## Installation
1. Use Python ≥3.9 and install PyTorch that matches your hardware.
2. (Optional) Create a virtual environment:
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```
3. Install the project in editable mode so the `models` package is importable. Installations default to the latest compatible releases, but for reproducible runs pin the Python and dependency versions listed in `pyproject.toml`/`requirements.txt` (e.g., by exporting a lockfile after testing):
   ```bash
   pip install --upgrade pip
   pip install -e .
   ```

### Version pinning & reproducibility
- The project targets Python 3.9+; develop with your preferred interpreter, but for complete reproducibility the Python version can be found at `.python-version`.
- Install pinned dependencies with `pip install -r requirements-pinned.txt` or install the latest versions with `pip install -e . --upgrade`.

### Environment variables
- Create a `.env` file at the repo root and set `HF_TOKEN=<your Hugging Face token>` (plus any overrides such as `TARGET_DATASET_ID`). Both the CLI and utility scripts call `python-dotenv` automatically, so the values are available without exporting them in your shell.

### Building the long-context source dataset
- Run `python scripts/create_long_context_source.py` to materialize `viktoroo/fineweb-edu-long-context-sample`. The script streams the `HuggingFaceFW/fineweb-edu` dataset (config `sample-10BT`), selects the first 1,024 rows whose `token_count` is at least 128,000 tokens, and pushes the result to the target dataset (defaults can be overridden via environment variables).

## Running a Capture
1. **Ensure instrumentation exists.** Out of the box Gemma 3 and Llama modules live under `models/` with sniffer hooks. To extend another model, copy its `transformers` implementation into `models/<name>/modeling_<name>.py` and add the `get_active_sniffer()` logic described below.
2. **Create a config.** Use `configs/sample_sniff.yaml` as a template and adjust dataset/model/tokenizer/capture/output sections.
3. **Launch the CLI.** Once installed, run either script form:
   ```bash
   sniff-qk --config configs/sample_sniff.yaml
   # or, if running from source without installation
   PYTHONPATH=. python sniff.py --config configs/sample_sniff.yaml
   ```
   At startup the CLI calls `patch_modeling_modules()` so your mirrored modules shadow `transformers`. It optionally downloads the current dataset snapshot (`output.hf_repo_id`) into `output.data_root`, runs inference + capture, then uploads the folder back to the Hub when finished.

## YAML Configuration Reference

```yaml
dataset:
  path: viktoroo/example-dataset
  name: null            # optional dataset config
  split: train
  text_column: text
  id_column: id         # optional logical ID per row
  max_samples: 128      # optional cap

model:
  name: google/gemma-2-2b
  revision: main
  dtype: float16        # float16|bfloat16|float32 aliases supported
  device_map: auto      # forwarded to transformers AutoModel
  trust_remote_code: false

tokenizer:
  name: google/gemma-2-2b
  max_length: 4096
  padding: longest

inference:
  batch_size: 2
  autocast_dtype: float16

capture:
  capture_queries: true
  capture_keys: true
  layers: null          # optional whitelist
  heads: null           # optional whitelist
  sampler:
    type: log_uniform
    base_rate: 1.0

output:
  data_root: data/sniffed-qk
  readme_path: README.md
  hf_repo_id: viktoroo/sniffed-qk       # leave null to stay local
```

**Field summary**
- `dataset.*` drives `datasets.load_dataset`. `max_samples` trims the split locally if you just want a quick smoke test.
- `model.*` feeds `AutoModelForCausalLM.from_pretrained`. `dtype` goes through `resolve_dtype`, and `device_map="auto"` enables multi-GPU sharding.
- `tokenizer.*` applies to `AutoTokenizer`. If you omit `name`, it falls back to `model.name`. The CLI forces `padding_side="right"` so padded tokens always sit at the end of the sequence and can be ignored safely.
- `inference.batch_size` controls the streaming iterator size; `autocast_dtype` only matters when CUDA is available.
- `capture.*` toggles which vectors flow to disk. `layers`/`heads` accept Python-style integer lists; pass nothing to capture every head. `sampler.type` currently supports `log_uniform` (custom samplers are pluggable; see below). The dataset split name always mirrors `model.name`.
- `output.data_root`/`readme_path` define where Parquet shards and the generated README live locally. If `readme_path` is relative it’s resolved inside `data_root`, so the README travels with the folder when uploading to the Hub. Set `hf_repo_id` to enable pull/push.

All bundled configs target the shared Hugging Face dataset `viktoroo/sniffed-qk`. Every run pulls the latest snapshot, writes new splits under sanitised model names, and pushes the combined folder so multiple checkpoints can coexist in one dataset.

### Included sample configs

| File | Model checkpoint | Max context | Notes |
| --- | --- | --- | --- |
| `configs/sample_sniff.yaml` | `google/gemma-2-2b` | 4,096 tokens | Demo config that matches the walkthrough above. |
| `configs/smollm2-135m.yaml` | `HuggingFaceTB/SmolLM2-135M` | 8,192 tokens | Smallest SmolLM2 baseline; ingests `viktoroo/fineweb-edu-long-context-sample`, batch size 4, publishes to `viktoroo/sniffed-qk` split `HuggingFaceTB_SmolLM2_135M`. |
| `configs/smollm2-360m.yaml` | `HuggingFaceTB/SmolLM2-360M` | 8,192 tokens | Medium SmolLM2; same dataset, batch size 2, pushes to the shared dataset split `HuggingFaceTB_SmolLM2_360M`. |
| `configs/smollm2-1.7b.yaml` | `HuggingFaceTB/SmolLM2-1.7B` | 8,192 tokens | Standard 1.7 B checkpoint; writes captures into the shared repo split `HuggingFaceTB_SmolLM2_1_7B`. |
| `configs/smollm2-2.2b-instruct.yaml` | `HuggingFaceTB/SmolLM2-2.2B-Instruct` | 16,384 tokens | 16 K instruct variant; publishes to `viktoroo/sniffed-qk` split `HuggingFaceTB_SmolLM2_2_2B_Instruct`. |

## Internals & Architecture

### Model patching & instrumentation
- `patch_modeling_modules(root=Path("models"))` walks every Python file under the given `models` root, temporarily prepends `root.parent` to `sys.path`, and aliases both package and module names into `sys.modules` under `transformers.models.*`. That means `transformers.models.llama.modeling_llama` now points at `models/llama/modeling_llama.py`.
- The CLI invokes this helper automatically, but you can call it manually in custom scripts (e.g., `sniff.patch_modeling_modules(root=Path("/tmp/my_models"))`).
- Inside each mirrored attention module, wrap the attention forward pass:
  ```python
  from sniffer import compute_positions, get_active_sniffer

  sniffer = get_active_sniffer()
  if sniffer is not None:
      positions = compute_positions(
          batch_size=query_states.shape[0],
          seq_len=query_states.shape[2],
          device=query_states.device,
          cache_position=cache_position,
      )
      sniffer.capture(
          layer_idx=self.layer_idx,
          query_states=query_states,
          key_states=key_states,
          positions=positions,
          sliding_window=self.sliding_window,
      )
  ```
- Use `set_active_example_ids([...])` inside your dataloader loop to ensure captures store the true IDs. If you skip it, the runtime defaults to `[0, 1, …]` per batch.
- Call `set_active_sequence_lengths([...])` (the CLI does this automatically) to provide the valid token counts for the current batch, ensuring padded positions are never captured regardless of batch padding length.

### Sniffer runtime & sampling
- `activate_sniffer(SnifferConfig)` opens a context manager around the capture session. `run_inference` does this once at startup, so all instrumentation simply grabs `get_active_sniffer()`.
- `Sniffer.capture` receives query/key tensors `(batch, heads, seq, dim)` and the per-token position tensor. It respects `layers`/`heads` filters and `capture_queries`/`capture_keys`.
- Sampling goes through the configured `Sampler`. Implementations take `positions` and log₂ `buckets` (calculated as `floor(log2(position + 1))`) and return a boolean mask per token. `LogUniformSampler` keeps `≈base_rate` tokens per bucket using a deterministic RNG seeded by `(example_id, layer_idx, head_idx, vector_kind)`, so reruns of the same data produce the same rows.

### Dataset saver & Hub sync
- `DatasetSaver` writes each `(model split, config)` pair to `data/<sanitized_split>/<config>/data.parquet`. Splits sanitize non-word characters (`meta/llama3-8b` → `meta_llama3_8b`). Configs are always `lXXhYY{q|k}`.
- Duplicate `(example_id, position)` rows are skipped automatically by keeping an in-memory cache per split/config and seeding it with previously written Parquet data. This lets you resume runs without rewriting identical entries.
- The saver also tracks per-model metadata (`source_dataset`, `dataset_split`, etc.) and bucket counts so the README can surface capture coverage.
- `pull_remote_dataset`/`push_remote_dataset` wrap `huggingface_hub.snapshot_download` and `api.upload_folder`. They respect `output.hf_repo_id`; leave it unset to avoid network calls.

## Dataset Columns

| Column | Description |
| --- | --- |
| `bucket` | Log₂ bucket identifier for the sampled position (`0` → first token, `1` → positions `[2,3]`, etc.). |
| `example_id` | ID supplied by `set_active_example_ids` or the implicit batch index. |
| `position` | Token index inside the example after any cache offset/truncation. |
| `vector` | Float32 array representing the captured key or query vector. |
| `sliding_window` | Optional sliding-window size for local attention layers (`null` for full causal attention). |

Every split/config folder only contains `data.parquet`, but the generated README front matter also includes aggregated configs:
- `all` / `all_q` / `all_k`
- `layerXX`, `layerXX_q`, `layerXX_k`
- Specific heads (`l00h00q`, `l00h03k`, …)

These entries reference glob patterns within `data_root`, so the Hugging Face Hub immediately exposes each subset.

## Loading Captures
```python
from datasets import load_dataset

# layer 0, head 0, queries captured from the gemma model
queries = load_dataset("viktoroo/sniffed-qk", "l00h00q", split="google_gemma3_9b")

# aggregated helpers exposed by the README front matter:
layer0_all_heads = load_dataset("viktoroo/sniffed-qk", "layer00", split="meta_llama3_8b")
all_queries = load_dataset("viktoroo/sniffed-qk", "all_q", split="meta_llama3_8b")
```
Remember that split names are sanitized (`/` and `-` become `_`). Inspect `ds_builder.config_names` or the README front matter to see what configs exist.

## Extending to New Models
1. Copy the relevant `transformers` module into `models/<model_name>/modeling_<model_name>.py`.
2. Import `compute_positions` + `get_active_sniffer` and wrap the attention block as shown above.
3. (Optional) store per-layer metadata (e.g., sliding window) on the module so you can pass it into `sniffer.capture`.
4. Run `pytest tests/test_models.py -k patch_modeling_modules` to ensure the aliasing still succeeds.
5. Execute your capture job; `patch_modeling_modules()` will map `transformers.models.<model_name>` to your local implementation automatically.

## Testing & Troubleshooting
- Run `pytest` (preferred) or narrow scopes like `pytest tests/test_sniffer.py -k sampler`.
- If `ModuleNotFoundError: transformers.models.foo` appears, confirm you ran `pip install -e .` (so the `models` package is importable) or call `sniff.patch_modeling_modules(root=Path("path/to/models"))` before importing transformers modules.
- `LogUniformSampler` is stochastic per token but deterministic per seed; if you expect denser sampling, raise `capture.sampler.base_rate`.
- When pushing to the Hub, the CLI overwrites the dataset repo contents. Use a scratch repo or branch if you want to keep older captures.

That’s it—create your config, run `sniff-qk`, and explore the captured attention vectors directly from Hugging Face datasets.

## Available Models
<!-- MODELS_START -->
- [HuggingFaceTB/SmolLM2-135M](https://huggingface.co/HuggingFaceTB/SmolLM2-135M)
  - dataset: viktoroo/longbench2-128k-plus (split: train)
  - buckets: b0=48, b1=46, b2=42, b3=51, b4=49, b5=38, b6=50, b7=45, b8=38, b9=35, b10=50, b11=46, b12=60
- [none](https://huggingface.co/none)
  - dataset: unknown (split: unknown)
  - buckets: (no samples)
<!-- MODELS_END -->

## Dataset Columns
<!-- COLUMNS_START -->
| Column | Description |
| --- | --- |
| `bucket` | Log2 bucket identifier used for sampling (lower buckets capture earlier positions). |
| `example_id` | Index of the example within the batch when the vector was captured. |
| `position` | Token position within the example's sequence (0-indexed). |
| `vector` | Float32 tensor containing the query or key vector; the config name encodes which. |
| `sliding_window` | Size of the sliding window for local attention (null implies global causal). |
<!-- COLUMNS_END -->

## Loading Examples
<!-- LOAD_START -->
1. Pick the configuration matching your target layer/head and vector type. For example, `l00h00q` captures queries from layer 0, head 0.
2. Use the source model identifier as the split. Splits follow the Hugging Face hub naming pattern (`org/name`).
3. Load the dataset via `datasets.load_dataset` with both the config and split:
```python
from datasets import load_dataset
ds = load_dataset("viktoroo/sniffed-qk", "l00h00q", split="org/name")
```
4. Convert to torch/tensorflow as needed; the `vector` column already stores float32 tensors.
<!-- LOAD_END -->
